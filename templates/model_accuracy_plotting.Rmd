---
title: "Plotting the measured model Accuracy"
date: "`r Sys.Date()`"
output:
  rmdformats::material:
    highlight: kate
---


```{r knitr_init, echo=FALSE, cache=FALSE}
library(knitr)
library(rmdformats)

## Global options
options(max.print = "75")
opts_chunk$set(echo = FALSE,
	           cache = FALSE,
               prompt = FALSE,
               tidy = TRUE,
               comment = NA,
               message = FALSE,
               warning = FALSE)
opts_knit$set(width = 75)
```


```{r warning=FALSE}
require(tidyverse)
require(purrr)
require(progress)
```

# Description


```{r}
resultsfiles <- dir(pattern = "^EP.*\\.RDS")
```


```{r}
plot_results <- function(mydata, given_name) {
    all_results <- purrr::map(
        mydata$models,
        function(x) x$model$results) 
    
    df_results <- purrr::map_df(
        mydata$models, 
        .id = "Model",
        function(x) x$model$results[,c("Accuracy", "Kappa")]) 
    
    tt <- purrr::map_df(
        mydata$models, 
        .id = "Model",
        function(x) dplyr::top_n(
            x$model$results[,c("Accuracy", "Kappa")], 
            1, Accuracy))
    
    tt <- arrange(tt, Accuracy)

    g1 <- df_results %>% 
        ggplot(aes(x = Accuracy, fill = Model)) + 
        geom_density(alpha = 0.2, adjust = 10) +
        theme_bw() + 
        ggtitle("Model Accuracy", subtitle = given_name)
    
    g2 <- df_results %>% 
        ggplot(aes(x = Accuracy, y = Kappa, colour = Model)) + 
        geom_point() +
        theme_bw() + 
        ggtitle("Model Accuracy", subtitle = given_name)
    
    g3 <- df_results %>% 
        ggplot(aes(y = Accuracy, x = Model, fill = Model, colour = Model)) + 
        geom_boxplot(alpha = 0.6) +
        theme_bw() +  coord_flip(ylim = c(0.8,1)) +
        ggtitle("Model Accuracy", subtitle = given_name)
    
    importance_dfs <- purrr::map(mydata$models, function(x) try(caret::varImp(x$model)))
    importance_dfs <- importance_dfs[map_chr(importance_dfs, class) != "try-error"]
    importance_dfs <- purrr::map_df(importance_dfs, .id = "Model", function(x) {
        tmp <- as.data.frame(x$importance)
        tmp$gene <- rownames(tmp)
        tmp <- tmp[tmp$Overall > 0, ]
        return(tmp)
        })
  
    importance_dfs_all <- importance_dfs
    
    importance_dfs <- importance_dfs %>% 
        group_by(Model) %>% 
        top_n(5, Overall) %>%
        arrange(desc(Overall)) %>%
        ungroup() %>%
        split(., .$Model)
    
    return(list(all_results = all_results,
                g1 = g1,g2 = g2,
                g3 = g3,tt = tt, 
                importance_dfs = importance_dfs,
                importance_dfs_all = importance_dfs_all,
                df_results = df_results))
}

```

```{r}
my_results <- vector("list")

pb <- progress::progress_bar$new(total = length(resultsfiles))
pb$tick(0)
for (i in resultsfiles) {
    mydata <- readRDS(i)
    
    my_results[[i]] <-try(plot_results(mydata, i))
    pb$tick()
    
    rm(mydata)
    gc()
}

```

# Plotting - Individual

## Boxplot of the prediction accuracies

```{r}
map(my_results, function(x) try(print(x$g3)))
```

# Plotting - Bundled

## Attempt to plot all of them at once

```{r fig.height=10, fig.width=6}
fails  <- map_lgl(my_results, function(x) class(try(x$df_results)) == "try-error") 
merged_accuracies <- map_df(my_results[!fails], function(x) try(x$df_results), .id = "dataset")

merged_accuracies <- merged_accuracies %>%
  separate(dataset,
           into = c("EP", "SUBSET", "FILT", "CLUSTER", "RDS")) 

merged_accuracies

default_plot <- function(x, name, val = c("Kappa", "Accuracy")) {
  aesthetics_kappa <- aes(y = Kappa, x = Model,
               group = interaction(Model, SUBSET), 
               fill = Model, colour = Model)
  aesthetics_acc <- aes(y = Accuracy, x = Model,
               group = interaction(Model, SUBSET), 
               fill = Model, colour = Model)
  aesthetics <- list(Kappa = aesthetics_acc, Accuracy = aesthetics_acc)
  g <- ggplot(x, aesthetics[[val]]) +
    facet_wrap(vars(SUBSET, CLUSTER), ncol = 1, strip.position = "left") +
    theme_bw() + 
    coord_flip() +
    ggtitle(name)
  return(g)
} 

default_plot(merged_accuracies, "Model Accuracy", 'Accuracy') +
    geom_boxplot(alpha = 0.6) 

default_plot(merged_accuracies, "Model Accuracy", 'Accuracy') +
    geom_violin(alpha = 0.6) 

default_plot(merged_accuracies, "Model Kappa", 'Kappa') +
    geom_boxplot(alpha = 0.6) 

default_plot(merged_accuracies, "Model Kappa", 'Kappa') +
    geom_violin(alpha = 0.6) 

```

# Importance DFs

## Importance data frames

I think it is important to find a way to represent this better ...

1. TODO correplation plot of the topN markers found by each method

```{r}
map(my_results, function(x) try(print(x$importance_dfs)))
```

# Random forest accuracy tendencies

Note how higher mtry values are favored when using extratrees as a metric but
not when using gini ....

```{r}

forest_data_all <- purrr::map(my_results, function(results) {
      try({ return(results$all_results$forest_unweighted)  })
    })

failed_ <- names(forest_data_all)[map_chr(forest_data_all, class) == "try-error"]
failed_ # TODO chech in what instanced do these fail ....

forest_data_all <- forest_data_all[!names(forest_data_all) %in% failed_]

forest_data_all  <- map_df(forest_data_all, function(x) x , .id = 'dataset')


forest_data_all[forest_data_all$splitrule == "gini", ] %>% 
            ggplot(aes(x = Accuracy, y = Kappa, colour = factor(mtry))) +
            geom_point() +
            facet_wrap(~ gsub("Allmodels_cluster", "", dataset)) + 
            viridis::scale_color_viridis(discrete = TRUE) +
            theme_bw() + ggtitle("Gini")
    

forest_data_all[forest_data_all$splitrule != "gini", ] %>% 
            ggplot(aes(x = Accuracy, y = Kappa, colour = factor(mtry))) +
            geom_point() +
            facet_wrap(~ gsub("Allmodels_cluster", "", dataset)) + 
            viridis::scale_color_viridis(discrete = TRUE) +
            theme_bw() + ggtitle("Extratrees")

```

# Conclusions


## Ideas on how to improve the benchmarking ...


1. .. add noise and check recapture of actually significant variables
2. Use the annotated dataset on the atlas.
3. Add train time to the measured variables. The current measured time is over all the tested parameters, not only fitting the model.

## Questions

1. How much can we downsample and still get good markers for prediction 
or how accurately can we get the optimal parameters 
(could drastically improve the speed)

2. How many trees are needed to reach stable importances of the top variables


```{r echo=FALSE, eval=FALSE}
tmp <- broom::tidy(mydata$glmnet_all$finalModel)

group_by(tmp, step) %>% 
  summarise(n = n())

tmp

lambda_diff <- abs(mydata$glmnet_all$bestTune$lambda - unique(tmp$lambda))
which_lambda_min <- which( min(lambda_diff) == lambda_diff )


dplyr::filter(tmp, lambda == unique(tmp$lambda)[which_lambda_min]) %>%
  arrange(desc(estimate))

tmp$lambda == which_lambda_min


mydata$glmnet_all$bestTune$lambda
```

